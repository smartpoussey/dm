import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier    

def load_data():
    df = pd.read_csv("/content/drug200.csv")
    df.fillna(df.mode().iloc[0], inplace=True)
    
    categorical_features = df.select_dtypes(include=['object']).columns.tolist()
    df[categorical_features] = df[categorical_features].apply(LabelEncoder().fit_transform)
    
    target_column = "Drug"
    X = df.drop(columns=[target_column])
    y = df[target_column]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    scaler = StandardScaler()
    return scaler.fit_transform(X_train), scaler.transform(X_test), y_train, y_test, X.columns

def plot_metrics(y_true, y_pred, model_name, param, feature_importance=None, feature_names=None):
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d')
    plt.title(f'Confusion Matrix: {model_name} ({param})')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    if feature_importance is not None and feature_names is not None:
        plt.figure(figsize=(10, 6))
        sorted_idx = np.argsort(feature_importance)
        plt.barh([feature_names[i] for i in sorted_idx], feature_importance[sorted_idx])
        plt.title(f'Feature Importance: {model_name} ({param})')
        plt.show()

def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, param, feature_names=None):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    print(f'\n{model_name} ({param})')
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred, zero_division=1))
    
    feature_importance = getattr(model, 'feature_importances_', None)
    plot_metrics(y_test, y_pred, model_name, param, feature_importance, feature_names)

def knn_best_k(X_train, X_test, y_train, y_test):
    knn_params = {'n_neighbors': list(range(1, 11))}
    knn = GridSearchCV(KNeighborsClassifier(), knn_params, scoring='accuracy', cv=3).fit(X_train, y_train).best_estimator_
    train_and_evaluate(knn, X_train, X_test, y_train, y_test, "KNN", f'Best k={knn.n_neighbors}')

def ensemble_classification(X_train, X_test, y_train, y_test):
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42),
        'AdaBoost': AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=42)
    }
    ensemble = VotingClassifier(estimators=[(name, model) for name, model in models.items()], voting='hard')
    train_and_evaluate(ensemble, X_train, X_test, y_train, y_test, "Ensemble", "Voting")

def boosting_classification(X_train, X_test, y_train, y_test):
    models = {
        'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),
        'CatBoost': CatBoostClassifier(n_estimators=50, verbose=0, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42),
        'LGBM': LGBMClassifier(n_estimators=50, random_state=42),
        'XGBoost': XGBClassifier(n_estimators=50, use_label_encoder=False, eval_metric='logloss', random_state=42)
    }
    for name, model in models.items():
        train_and_evaluate(model, X_train, X_test, y_train, y_test, name, "Boosting")

def stacking_classification(X_train, X_test, y_train, y_test):
    base_models = [
        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('SVM', SVC(probability=True, random_state=42)),
        ('NaiveBayes', GaussianNB())
    ]
    stack = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())
    train_and_evaluate(stack, X_train, X_test, y_train, y_test, "Stacking", "Meta-Logistic")

def cascading_classification(X_train, X_test, y_train, y_test):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    X_train_ext = np.hstack((X_train, rf.predict(X_train).reshape(-1, 1)))
    X_test_ext = np.hstack((X_test, rf.predict(X_test).reshape(-1, 1)))
    train_and_evaluate(LogisticRegression(), X_train_ext, X_test_ext, y_train, y_test, "Cascading", "RF+Logistic")

X_train, X_test, y_train, y_test, feature_names = load_data()

models = {
    'KNN': KNeighborsClassifier(n_neighbors=3),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)
}

for name, model in models.items():
    train_and_evaluate(model, X_train, X_test, y_train, y_test, name, "Default", feature_names)

knn_best_k(X_train, X_test, y_train, y_test)
ensemble_classification(X_train, X_test, y_train, y_test)
boosting_classification(X_train, X_test, y_train, y_test)
cascading_classification(X_train, X_test, y_train, y_test)
stacking_classification(X_train, X_test, y_train, y_test)


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering

from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, LabelEncoder

def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath).dropna(how='all')

    categorical_features = df.select_dtypes(include=['object']).columns
    for feature in categorical_features:
        df[feature] = LabelEncoder().fit_transform(df[feature].astype(str))

    df.fillna(df.mean(), inplace=True)
    X_scaled = StandardScaler().fit_transform(df)

    return X_scaled

def Visualization(X, labels, cluster_type):
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette='Set1')
    plt.title(f'{cluster_type} Clustering Visualization')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()

# Find optimal k using Elbow Method
def find_optimal_k(X, max_k=10):
    distortions = []
    for k in range(1, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        distortions.append(kmeans.inertia_)
    
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, max_k + 1), distortions, marker='o')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Inertia')
    plt.title('Elbow Method for Optimal k')
    plt.show()

# K-Means Clustering
def perform_kmeans_clustering(X, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    
    print(f"\nK-Means Clustering with {n_clusters} Clusters")
    print(f"Silhouette Score: {silhouette_score(X, labels):.4f}")
    
    Visualization(X, labels, "K-Means")


# Hierarchical Clustering
def perform_hierarchical_clustering(X, n_clusters):
    hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    labels = hierarchical.fit_predict(X)

    print(f"\nHierarchical Clustering with {n_clusters} Clusters")
    print(f"Silhouette Score: {silhouette_score(X, labels):.4f}")

    Visualization(X, labels, "Hierarchical")

    plt.figure(figsize=(10, 7))
    dendrogram(linkage(X, method='ward'))
    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('Sample Index')
    plt.ylabel('Distance')
    plt.show()


# DBSCAN Clustering
def perform_dbscan_clustering(X, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels = dbscan.fit_predict(X)
    
    print("\nDBSCAN Clustering")
    print(f"Clusters Found: {len(set(labels)) - (1 if -1 in labels else 0)}")
    print(f"Silhouette Score: {silhouette_score(X, labels) if len(set(labels)) > 1 else 'N/A'}")

    Visualization(X, labels, "DBSCAN")
    

X_scaled = load_and_preprocess_data('/content/drug200.csv')
find_optimal_k(X_scaled)

perform_kmeans_clustering(X_scaled, n_clusters=3)
perform_hierarchical_clustering(X_scaled, n_clusters=2)
perform_dbscan_clustering(X_scaled, eps=0.7, min_samples=5)
